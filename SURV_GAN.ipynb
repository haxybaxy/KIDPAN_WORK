{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss D: 90.41207122802734 | Loss G: 0.9191253185272217\n",
      "Epoch 1000 | Loss D: 100.03914642333984 | Loss G: 0.0\n",
      "Epoch 2000 | Loss D: 100.00851440429688 | Loss G: 0.0\n",
      "Epoch 3000 | Loss D: 100.0 | Loss G: 0.0\n",
      "Epoch 4000 | Loss D: 100.0 | Loss G: 0.0\n",
      "Epoch 5000 | Loss D: 100.0 | Loss G: 0.0\n",
      "Epoch 6000 | Loss D: 100.0 | Loss G: 0.0\n",
      "Epoch 7000 | Loss D: 100.0 | Loss G: 0.0\n",
      "Epoch 8000 | Loss D: 100.0 | Loss G: 0.0\n",
      "Epoch 9000 | Loss D: 100.0 | Loss G: 0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All samples are censored",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 141\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Calculate the Concordance Index (C-index) on the synthetic data compared to the real data\u001b[39;00m\n\u001b[0;32m    140\u001b[0m real_cindex \u001b[38;5;241m=\u001b[39m concordance_index_censored(E \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, T, T)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 141\u001b[0m synthetic_cindex \u001b[38;5;241m=\u001b[39m \u001b[43mconcordance_index_censored\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynthetic_event\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthetic_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthetic_time\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Data C-index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreal_cindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSynthetic Data C-index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msynthetic_cindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Omar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sksurv\\metrics.py:214\u001b[0m, in \u001b[0;36mconcordance_index_censored\u001b[1;34m(event_indicator, event_time, estimate, tied_tol)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcordance_index_censored\u001b[39m(event_indicator, event_time, estimate, tied_tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m):\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Concordance index for right-censored data\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m    The concordance index is defined as the proportion of all comparable pairs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m           Statistics in Medicine, 15(4), 361-87, 1996.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     event_indicator, event_time, estimate \u001b[38;5;241m=\u001b[39m \u001b[43m_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_indicator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(estimate)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _estimate_concordance_index(event_indicator, event_time, estimate, w, tied_tol)\n",
      "File \u001b[1;32mc:\\Users\\Omar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sksurv\\metrics.py:58\u001b[0m, in \u001b[0;36m_check_inputs\u001b[1;34m(event_indicator, event_time, estimate)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed a minimum of two samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m event_indicator\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll samples are censored\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m event_indicator, event_time, estimate\n",
      "\u001b[1;31mValueError\u001b[0m: All samples are censored"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# ---------------------------------------------\n",
    "# 1. Load your own survival dataset\n",
    "# Replace this section with your real dataset\n",
    "# ---------------------------------------------\n",
    "mydata = pd.read_csv(\"data_ready_45.csv\")\n",
    "# Example of simulated data (you should replace this with your actual data)\n",
    "np.random.seed(42)  # Remove this line if you don't need random seeding\n",
    "\n",
    "# --- Replace the following three variables with your actual data ---\n",
    "X = mydata.drop(columns=[\"GRF_STAT_PA\", \"time_frame\"], axis=1)\n",
    "T = mydata.pop(\"time_frame\")\n",
    "E = mydata.pop(\"GRF_STAT_PA\")\n",
    "\n",
    "X = X.to_numpy()\n",
    "T = T.to_numpy()\n",
    "E = E.to_numpy()\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)  # Covariates (features)\n",
    "T_tensor = torch.tensor(T, dtype=torch.float32)  # Time-to-event (survival times)\n",
    "E_tensor = torch.tensor(E, dtype=torch.float32)  # Event/censoring indicator\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. No need to change the models (Generator, Discriminator)\n",
    "# You can leave the architecture as it is unless you want to tweak it.\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Define the Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)  # Output: time-to-event and event/censoring indicator\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        output = self.fc(z)\n",
    "        time = torch.relu(output[:, 0])  # Ensure non-negative times\n",
    "        event = torch.sigmoid(output[:, 1])  # Probability of event (between 0 and 1)\n",
    "        return time, event\n",
    "\n",
    "# Define the Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Output: probability of being real\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize the generator and discriminator\n",
    "generator = Generator(input_dim=100, output_dim=2)  # Input: noise vector, Output: time-to-event and event/censoring\n",
    "discriminator = Discriminator(input_dim=2)  # Input: time-to-event and event/censoring pairs\n",
    "\n",
    "# Define the loss function and optimizers\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy for real/fake classification\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Training loop - No need to change unless you'd like to tweak epochs or batch size\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Training loop for GAN\n",
    "epochs = 10000  # You can reduce this if you don't need that many epochs\n",
    "batch_size = 64\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train Discriminator with real data\n",
    "    discriminator.zero_grad()\n",
    "    \n",
    "    # --- Replace this sampling section if needed ---\n",
    "    # Sample a batch of real data (time-to-event and event/censoring)\n",
    "    idx = np.random.randint(0, len(T), batch_size)  # Adjust this to your dataset's size\n",
    "    real_time = T_tensor[idx].view(-1, 1)\n",
    "    real_event = E_tensor[idx].view(-1, 1)\n",
    "    real_data = torch.cat([real_time, real_event], dim=1)\n",
    "    real_labels = torch.full((batch_size, 1), real_label, dtype=torch.float32)     # -----------------------------------------------\n",
    "    \n",
    "    output_real = discriminator(real_data)\n",
    "    loss_real = criterion(output_real, real_labels)\n",
    "    \n",
    "    # Generate fake data using the Generator\n",
    "    z = torch.randn(batch_size, 100)  # Random noise input\n",
    "    fake_time, fake_event = generator(z)\n",
    "    fake_data = torch.cat([fake_time.view(-1, 1), fake_event.view(-1, 1)], dim=1)\n",
    "    fake_labels = torch.full((batch_size, 1), fake_label, dtype=torch.float32)    \n",
    "    # Train Discriminator on fake data\n",
    "    output_fake = discriminator(fake_data.detach())\n",
    "    loss_fake = criterion(output_fake, fake_labels)\n",
    "    \n",
    "    # Combine discriminator loss\n",
    "    loss_D = loss_real + loss_fake\n",
    "    loss_D.backward()\n",
    "    optimizer_D.step()\n",
    "    \n",
    "    # Train Generator to fool the discriminator\n",
    "    generator.zero_grad()\n",
    "    output_fake = discriminator(fake_data)\n",
    "    loss_G = criterion(output_fake, real_labels)  # Goal: make discriminator classify fake data as real\n",
    "    loss_G.backward()\n",
    "    optimizer_G.step()\n",
    "    \n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch} | Loss D: {loss_D.item()} | Loss G: {loss_G.item()}')\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. Evaluation: Adjust this section based on your dataset size and evaluation requirements\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Generate synthetic data using the trained Generator\n",
    "z = torch.randn(len(T), 100)  # Adjust this to the number of synthetic samples you want to generate\n",
    "synthetic_time, synthetic_event = generator(z)\n",
    "synthetic_time = synthetic_time.detach().numpy()\n",
    "synthetic_event = (synthetic_event.detach().numpy() > 0.5).astype(int)  # Convert event probabilities to binary\n",
    "\n",
    "# Calculate the Concordance Index (C-index) on the synthetic data compared to the real data\n",
    "real_cindex = concordance_index_censored(E == 1, T, T)[0]\n",
    "synthetic_cindex = concordance_index_censored(synthetic_event == 1, synthetic_time, synthetic_time)[0]\n",
    "\n",
    "print(f\"Real Data C-index: {real_cindex}\")\n",
    "print(f\"Synthetic Data C-index: {synthetic_cindex}\")\n",
    "\n",
    "# Visualization: Compare the distribution of real vs. synthetic survival times\n",
    "plt.hist(T, bins=50, alpha=0.5, label='Real Data')  # Real time-to-event data\n",
    "plt.hist(synthetic_time, bins=50, alpha=0.5, label='Synthetic Data')  # Synthetic time-to-event data\n",
    "plt.legend()\n",
    "plt.title('Comparison of Real vs. Synthetic Survival Times')\n",
    "plt.xlabel('Time-to-Event')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
